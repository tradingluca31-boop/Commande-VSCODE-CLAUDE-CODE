# -*- coding: utf-8 -*-
"""
================================================================================
ğŸ¤– L'AGENT 7 PARLE - Interview Automatique
================================================================================

L'agent rÃ©pond AUTOMATIQUEMENT Ã  toutes les questions.
Pas besoin d'interaction - il s'explique tout seul!

================================================================================
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
from datetime import datetime
from collections import Counter
import time
import warnings
warnings.filterwarnings('ignore')

# Add paths
project_root = Path("C:/Users/lbye3/Desktop/GoldRL")
parent_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / 'src'))
sys.path.append(str(project_root / 'AGENT_V2'))
sys.path.append(str(parent_dir))

import config
from data_loader import DataLoader
from feature_engineering import FeatureEngineering
from environment.trading_env_v2_ultimate import GoldTradingEnv

from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

def slow_print(text, delay=0.02):
    """Print text character by character for effect"""
    for char in text:
        print(char, end='', flush=True)
        time.sleep(delay)
    print()

def agent_says(text):
    """Format agent speech"""
    print(f"\nğŸ¤– Agent 7: {text}")

def interviewer_asks(text):
    """Format interviewer question"""
    print(f"\nğŸ‘¤ Toi: {text}")

print("="*70)
print("ğŸ¤ INTERVIEW AUTOMATIQUE - AGENT 7 V2.1 PARLE")
print("="*70)
print("\nChargement de l'agent... (30 secondes)")

# ============================================================================
# LOAD
# ============================================================================
loader = DataLoader(verbose=False)
aligned_df, auxiliary_data = loader.load_all_data()
fe = FeatureEngineering(aligned_df, auxiliary_data, verbose=False)
features = fe.compute_all_features(config.TEST_START_DATE, config.TEST_END_DATE)
prices = auxiliary_data['xauusd_raw']['H1'].loc[features.index, ['open', 'high', 'low', 'close']]

env = GoldTradingEnv(
    features_df=features,
    prices_df=prices,
    initial_balance=100_000,
    action_space_type='discrete',
    verbose=False
)
env = Monitor(env)
vec_env = DummyVecEnv([lambda: env])

model_paths = [
    parent_dir / "models" / "checkpoints" / "agent7_critic_boost_lstm_150000_steps.zip",
    parent_dir / "models" / "best_model.zip",
]
model = None
for path in model_paths:
    if path.exists():
        model = RecurrentPPO.load(path, env=vec_env)
        break

def get_unwrapped_env(vec_env):
    env = vec_env.envs[0]
    while hasattr(env, 'env'):
        env = env.env
    return env

obs = vec_env.reset()
lstm_states = None
raw_env = get_unwrapped_env(vec_env)

print("\nâœ… Agent chargÃ©! L'interview commence...\n")
time.sleep(1)

# ============================================================================
# INTERVIEW AUTOMATIQUE
# ============================================================================

print("="*70)
print("ğŸ¤ DÃ‰BUT DE L'INTERVIEW")
print("="*70)

# Question 1
interviewer_asks("Agent 7, quelle action vas-tu prendre maintenant?")
time.sleep(0.5)

# Get probabilities
action_counts = Counter()
for _ in range(50):
    action, _ = model.predict(obs, state=lstm_states, deterministic=False)
    action_counts[int(action[0])] += 1
probs = {k: v/50 for k, v in action_counts.items()}

# Deterministic action
action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)
action_int = int(action[0])
action_names = {0: "SELL", 1: "HOLD", 2: "BUY"}

agent_says(f"Je choisis {action_names[action_int]}.")
print(f"\n   Mes probabilitÃ©s:")
print(f"   - SELL: {probs.get(0,0)*100:.0f}%")
print(f"   - HOLD: {probs.get(1,0)*100:.0f}%")
print(f"   - BUY:  {probs.get(2,0)*100:.0f}%")

time.sleep(1)

# Question 2
interviewer_asks("Pourquoi tu choisis cette action?")
time.sleep(0.5)

if probs.get(1,0) > 0.7:
    agent_says("HonnÃªtement? J'ai peur de perdre de l'argent.")
    print("\n   Je prÃ©fÃ¨re HOLD parce que:")
    print("   - Si je ne fais rien, je ne perds pas")
    print("   - BUY et SELL sont risquÃ©s")
    print("   - J'ai appris que HOLD = sÃ©curitÃ©")
elif probs.get(1,0) > 0.5:
    agent_says("Je suis prudent. J'attends une meilleure opportunitÃ©.")
else:
    agent_says("Je vois une opportunitÃ© de trading!")

time.sleep(1)

# Question 3
interviewer_asks("As-tu peur de perdre de l'argent?")
time.sleep(0.5)

if probs.get(1,0) > 0.8:
    agent_says("OUI, j'ai trÃ¨s peur! ğŸ˜°")
    print("\n   Je choisis HOLD", f"{probs.get(1,0)*100:.0f}% du temps.")
    print("   Mon raisonnement:")
    print("   'Si je ne trade pas, je ne peux pas perdre'")
    print("   'Le marchÃ© est dangereux, mieux vaut attendre'")
elif probs.get(1,0) > 0.5:
    agent_says("Un peu... Je suis prudent.")
else:
    agent_says("Non, je suis confiant! ğŸ˜")

time.sleep(1)

# Question 4
interviewer_asks("Quelle est ta position actuelle?")
time.sleep(0.5)

pos = raw_env.position_side
pos_names = {-1: "SHORT (vente)", 0: "FLAT (aucune)", 1: "LONG (achat)"}
agent_says(f"Ma position: {pos_names[pos]}")
print(f"\n   Balance: ${raw_env.balance:,.2f}")
print(f"   Drawdown: {raw_env.max_drawdown*100:.2f}%")
print(f"   Trades effectuÃ©s: {len(raw_env.trades)}")

time.sleep(1)

# Question 5
interviewer_asks("Combien de trades as-tu fait?")
time.sleep(0.5)

n_trades = len(raw_env.trades)
if n_trades == 0:
    agent_says("Aucun trade! ğŸ˜•")
    print("\n   Je n'ai ouvert aucune position.")
    print("   C'est peut-Ãªtre parce que j'ai peur de perdre...")
else:
    agent_says(f"J'ai fait {n_trades} trades.")

time.sleep(1)

# Question 6: Test sur 100 steps
interviewer_asks("Montre-moi ce que tu fais sur 100 steps.")
time.sleep(0.5)

agent_says("D'accord, je vais te montrer...")

obs = vec_env.reset()
lstm_states = None
actions_100 = []

print(f"\n   Mes actions sur 100 steps:")
for step in range(100):
    action, lstm_states = model.predict(obs, state=lstm_states, deterministic=False)
    action_int = int(action[0])
    actions_100.append(action_int)
    obs, _, done, _ = vec_env.step(action)
    if done[0]:
        obs = vec_env.reset()
        lstm_states = None

counts = Counter(actions_100)
print(f"   - SELL: {counts.get(0,0)} fois ({counts.get(0,0)}%)")
print(f"   - HOLD: {counts.get(1,0)} fois ({counts.get(1,0)}%)")
print(f"   - BUY:  {counts.get(2,0)} fois ({counts.get(2,0)}%)")

time.sleep(1)

# Question 7
interviewer_asks("Pourquoi tu ne trades presque pas?")
time.sleep(0.5)

hold_pct = counts.get(1,0)
if hold_pct > 80:
    agent_says("Parce que j'ai appris que HOLD est la meilleure option! ğŸ˜°")
    print("\n   Voici mon raisonnement:")
    print("   1. Quand je trade, parfois je perds de l'argent")
    print("   2. Quand je fais HOLD, je ne perds rien")
    print("   3. Donc HOLD = meilleure option (pour moi)")
    print("")
    print("   Le problÃ¨me:")
    print("   - Mon reward pour HOLD â‰ˆ 0 (neutre)")
    print("   - Mon reward pour TRADE = risque de nÃ©gatif")
    print("   - J'ai appris Ã  Ã©viter le risque")
elif hold_pct > 50:
    agent_says("Je suis prudent, j'attends les bonnes opportunitÃ©s.")
else:
    agent_says("Je trade quand je vois une opportunitÃ©!")

time.sleep(1)

# Question 8
interviewer_asks("Comment peut-on t'aider Ã  trader plus?")
time.sleep(0.5)

agent_says("Les paramÃ¨tres V3 AGGRESSIVE sont dÃ©jÃ  en place!")
print("""
   V3 AGGRESSIVE - DÃ‰JÃ€ IMPLÃ‰MENTÃ‰:

   1. ğŸ BONUS TRADING AGRESSIF
      â†’ +0.08 bonus pour chaque BUY ou SELL (was +0.03)
      â†’ E[reward|TRADE] >> E[reward|HOLD]

   2. ğŸ˜¤ PÃ‰NALITÃ‰ HOLD IMMÃ‰DIATE
      â†’ -0.02 penalty dÃ¨s le 1er HOLD (was aprÃ¨s 30)
      â†’ -0.05 extra aprÃ¨s 10 HOLDs consÃ©cutifs

   3. ğŸ² ENTROPY MAXIMUM
      â†’ ent_coef = 0.35 (was 0.25)
      â†’ Minimum = 0.15 (was 0.12)
      â†’ High phase = 60% (was 50%)

   4. ğŸ§  BESOIN DE RÃ‰-ENTRAÃNER FROM SCRATCH
      â†’ Le modÃ¨le actuel a appris les mauvaises habitudes
      â†’ Il faut relancer LAUNCH_TRAINING_500K.bat
""")

time.sleep(1)

# Conclusion
print("\n" + "="*70)
print("ğŸ“‹ RÃ‰SUMÃ‰ DE L'INTERVIEW")
print("="*70)

agent_says("Voici mon auto-diagnostic:")
print(f"""
   ğŸ” CE QUE J'AI RÃ‰VÃ‰LÃ‰:

   - Distribution: {counts.get(1,0)}% HOLD ({"TROP!" if counts.get(1,0) > 80 else "OK"})
   - Trades ouverts: {n_trades}
   - Peur de perdre: {"OUI" if probs.get(1,0) > 0.7 else "Non"}
   - Cause: Critic V(s) = 0 (MORT)

   ğŸ› ï¸ V3 AGGRESSIVE DÃ‰JÃ€ IMPLÃ‰MENTÃ‰:

   âœ… Entropy: 0.35 (was 0.25) - MAXIMUM
   âœ… Entropy minimum: 0.15 (was 0.12)
   âœ… High phase: 60% (was 50%)
   âœ… Bonus trading: +0.08 pour BUY/SELL (was +0.03)
   âœ… Penalty HOLD: -0.02 IMMÃ‰DIAT + -0.05 aprÃ¨s 10
   âœ… Penalty collapse: -0.05 Ã  -0.10 (was -0.03)

   ğŸš€ PROCHAINE Ã‰TAPE:

   RÃ©-entraÃ®ner FROM SCRATCH avec V3 AGGRESSIVE!

   cd "C:\\Users\\lbye3\\Desktop\\GoldRL\\AGENT\\AGENT 7\\ENTRAINEMENT\\FICHIER IMPORTANT AGENT 7\\launchers"
   RUN_QUICK_TEST_10K.bat   (validation rapide ~5min)
   LAUNCH_TRAINING_500K.bat (training complet ~6h)
""")

print("="*70)
print("ğŸ¤ FIN DE L'INTERVIEW")
print("="*70)

vec_env.close()

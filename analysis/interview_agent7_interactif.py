# -*- coding: utf-8 -*-
"""
================================================================================
INTERVIEW INTERACTIF - AGENT 7 V2.1
================================================================================

Pose des questions directement √† l'agent et obtiens ses r√©ponses!

L'agent "r√©pond" en montrant:
- Ses probabilit√©s d'action (ce qu'il PENSE)
- Son action choisie (ce qu'il FAIT)
- Sa Value Function (comment il √âVALUE la situation)
- Les features qui l'influencent (POURQUOI il d√©cide)

================================================================================
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
from datetime import datetime
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Add paths
project_root = Path("C:/Users/lbye3/Desktop/GoldRL")
parent_dir = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))
sys.path.append(str(project_root / 'src'))
sys.path.append(str(project_root / 'AGENT_V2'))
sys.path.append(str(parent_dir))

import config
from data_loader import DataLoader
from feature_engineering import FeatureEngineering
from environment.trading_env_v2_ultimate import GoldTradingEnv

from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

import torch

# ============================================================================
# SETUP
# ============================================================================

def clear_screen():
    import os
    os.system('cls' if os.name == 'nt' else 'clear')

def print_header():
    print("\n" + "="*70)
    print("   ü§ñ INTERVIEW AGENT 7 V2.1 - CRITIC BOOST + LSTM")
    print("="*70)

def get_unwrapped_env(vec_env):
    env = vec_env.envs[0]
    while hasattr(env, 'env'):
        env = env.env
    return env

# ============================================================================
# LOAD EVERYTHING
# ============================================================================

print("Chargement de l'agent en cours...")
print("(Cela peut prendre 30 secondes)")

# Load data
loader = DataLoader(verbose=False)
aligned_df, auxiliary_data = loader.load_all_data()
fe = FeatureEngineering(aligned_df, auxiliary_data, verbose=False)
features = fe.compute_all_features(config.TEST_START_DATE, config.TEST_END_DATE)
prices = auxiliary_data['xauusd_raw']['H1'].loc[features.index, ['open', 'high', 'low', 'close']]

# Create environment
env = GoldTradingEnv(
    features_df=features,
    prices_df=prices,
    initial_balance=100_000,
    action_space_type='discrete',
    verbose=False
)
env_monitor = Monitor(env)
vec_env = DummyVecEnv([lambda: env_monitor])

# Load model
model_paths = [
    parent_dir / "models" / "checkpoints" / "agent7_critic_boost_lstm_150000_steps.zip",
    parent_dir / "models" / "best_model.zip",
    parent_dir / "models" / "agent7_critic_boost_lstm_final.zip"
]

model = None
model_name = "Unknown"
for path in model_paths:
    if path.exists():
        model = RecurrentPPO.load(path, env=vec_env)
        model_name = path.name
        break

if model is None:
    print("ERREUR: Aucun mod√®le trouv√©!")
    sys.exit(1)

# Initialize
obs = vec_env.reset()
lstm_states = None
raw_env = get_unwrapped_env(vec_env)

print(f"\n‚úÖ Agent charg√©: {model_name}")
print("‚úÖ Environnement pr√™t")

# ============================================================================
# INTERVIEW FUNCTIONS
# ============================================================================

def ask_action():
    """Demande: Quelle action vas-tu prendre?"""
    global obs, lstm_states

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Quelle action vas-tu prendre maintenant?")
    print("-"*50)

    # Get probabilities by sampling
    action_counts = Counter()
    for _ in range(100):
        action, _ = model.predict(obs, state=lstm_states, deterministic=False)
        action_counts[int(action[0])] += 1

    probs = {k: v/100 for k, v in action_counts.items()}

    # Get deterministic action
    action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)
    action_int = int(action[0])
    action_names = {0: "SELL üìâ", 1: "HOLD ‚è∏Ô∏è", 2: "BUY üìà"}

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   Ma d√©cision: {action_names[action_int]}")
    print(f"")
    print(f"   Mes probabilit√©s:")
    print(f"   ‚îú‚îÄ‚îÄ SELL: {'‚ñà' * int(probs.get(0,0)*20):<20} {probs.get(0,0)*100:.1f}%")
    print(f"   ‚îú‚îÄ‚îÄ HOLD: {'‚ñà' * int(probs.get(1,0)*20):<20} {probs.get(1,0)*100:.1f}%")
    print(f"   ‚îî‚îÄ‚îÄ BUY:  {'‚ñà' * int(probs.get(2,0)*20):<20} {probs.get(2,0)*100:.1f}%")

    # Interpretation
    max_prob = max(probs.values()) if probs else 0
    if max_prob > 0.9:
        print(f"\n   ‚ö†Ô∏è Je suis TR√àS s√ªr de moi ({max_prob*100:.0f}%)")
        print(f"      Risque de mode collapse!")
    elif max_prob > 0.6:
        print(f"\n   üí≠ J'ai une pr√©f√©rence claire ({max_prob*100:.0f}%)")
    else:
        print(f"\n   ü§î J'h√©site entre plusieurs options")

    return action_int

def ask_why():
    """Demande: Pourquoi cette d√©cision?"""
    global obs

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Pourquoi cette d√©cision?")
    print("-"*50)

    obs_flat = obs.flatten()

    # Get top influential features
    top_indices = np.argsort(np.abs(obs_flat))[::-1][:10]

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   Les features qui m'influencent le plus:")
    print(f"")

    for i, idx in enumerate(top_indices[:5]):
        val = obs_flat[idx]
        direction = "‚Üë haut" if val > 0 else "‚Üì bas"
        print(f"   {i+1}. Feature[{idx}]: {val:+.3f} ({direction})")

    # Check RL features
    print(f"\n   Mon √©tat interne (RL features):")
    rl_start = 209
    print(f"   ‚îú‚îÄ‚îÄ Position actuelle: {obs_flat[220]:.0f} (-1=SHORT, 0=FLAT, 1=LONG)")
    print(f"   ‚îú‚îÄ‚îÄ Regret signal: {obs_flat[212]:.3f}")
    print(f"   ‚îú‚îÄ‚îÄ Volatility: {obs_flat[219]:.3f}")
    print(f"   ‚îî‚îÄ‚îÄ Win rate r√©cent: {obs_flat[222]:.3f}")

def ask_fear():
    """Demande: As-tu peur de perdre?"""
    global obs, lstm_states

    print("\n" + "-"*50)
    print("‚ùì QUESTION: As-tu peur de perdre de l'argent?")
    print("-"*50)

    # Analyze HOLD preference
    action_counts = Counter()
    for _ in range(100):
        action, _ = model.predict(obs, state=lstm_states, deterministic=False)
        action_counts[int(action[0])] += 1

    hold_pct = action_counts.get(1, 0) / 100

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")

    if hold_pct > 0.8:
        print(f"   üò∞ OUI, j'ai tr√®s peur!")
        print(f"   Je choisis HOLD {hold_pct*100:.0f}% du temps")
        print(f"   HOLD = pas de risque = pas de perte")
        print(f"")
        print(f"   üí≠ Mon raisonnement:")
        print(f"   'Si je ne trade pas, je ne peux pas perdre'")
        print(f"   'Le march√© est dangereux, mieux vaut attendre'")
    elif hold_pct > 0.5:
        print(f"   üòê Un peu, je suis prudent")
        print(f"   Je choisis HOLD {hold_pct*100:.0f}% du temps")
        print(f"   Je pr√©f√®re attendre les bonnes opportunit√©s")
    else:
        print(f"   üòé Non, je suis confiant!")
        print(f"   Je ne choisis HOLD que {hold_pct*100:.0f}% du temps")
        print(f"   Je n'ai pas peur de prendre des risques calcul√©s")

def ask_position():
    """Demande: Quelle est ta position actuelle?"""
    global raw_env

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Quelle est ta position actuelle?")
    print("-"*50)

    pos = raw_env.position_side
    pos_names = {-1: "SHORT üìâ", 0: "FLAT (pas de position)", 1: "LONG üìà"}

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   Position: {pos_names[pos]}")
    print(f"   Balance: ${raw_env.balance:,.2f}")
    print(f"   Equity: ${raw_env.equity:,.2f}")
    print(f"   Drawdown: {raw_env.max_drawdown*100:.2f}%")
    print(f"   Trades effectu√©s: {len(raw_env.trades)}")

    if pos == 0:
        print(f"\n   üí≠ Je n'ai pas de position ouverte")
        print(f"      J'attends une opportunit√©...")
    else:
        print(f"\n   üí≠ Je suis en position {pos_names[pos]}")
        print(f"      Entry price: ${raw_env.entry_price:.2f}")

def ask_memory():
    """Demande: Que retiens-tu de tes trades pass√©s?"""
    global raw_env, obs

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Que retiens-tu de tes trades pass√©s?")
    print("-"*50)

    n_trades = len(raw_env.trades)
    obs_flat = obs.flatten()

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   Nombre de trades en m√©moire: {n_trades}")

    if n_trades == 0:
        print(f"\n   üòï Je n'ai aucun souvenir de trades!")
        print(f"      C'est un COLD START")
        print(f"      Mes MEMORY features sont √† valeur par d√©faut")
        print(f"")
        print(f"   Mes features m√©moire actuelles:")
        print(f"   ‚îú‚îÄ‚îÄ Win rate: {obs_flat[222]:.2f} (d√©faut: 0.5)")
        print(f"   ‚îú‚îÄ‚îÄ Streak: {obs_flat[223]:.2f} (d√©faut: 0)")
        print(f"   ‚îú‚îÄ‚îÄ Avg PnL: {obs_flat[224]:.2f} (d√©faut: 0)")
        print(f"   ‚îî‚îÄ‚îÄ Best/Worst: {obs_flat[225]:.2f}/{obs_flat[226]:.2f}")
    else:
        wins = sum(1 for t in raw_env.trades if t['pnl'] > 0)
        losses = n_trades - wins

        print(f"\n   üìä Mon historique:")
        print(f"   ‚îú‚îÄ‚îÄ Wins: {wins}")
        print(f"   ‚îú‚îÄ‚îÄ Losses: {losses}")
        print(f"   ‚îî‚îÄ‚îÄ Win Rate: {wins/n_trades*100:.1f}%")

        if n_trades >= 3:
            recent = raw_env.trades[-3:]
            print(f"\n   üìà Mes 3 derniers trades:")
            for i, t in enumerate(recent):
                result = "‚úÖ WIN" if t['pnl'] > 0 else "‚ùå LOSS"
                print(f"   {i+1}. {result}: ${t['pnl']:.2f}")

def ask_execute_step():
    """Ex√©cute un step et montre ce qui se passe"""
    global obs, lstm_states, raw_env

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Montre-moi un step complet")
    print("-"*50)

    # Before
    pos_before = raw_env.position_side
    balance_before = raw_env.balance

    # Get action
    action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)
    action_int = int(action[0])
    action_names = {0: "SELL", 1: "HOLD", 2: "BUY"}

    print(f"\nü§ñ EX√âCUTION DU STEP:")
    print(f"")
    print(f"   1Ô∏è‚É£ Je re√ßois l'observation (229 features)")
    print(f"   2Ô∏è‚É£ Mon LSTM traite la s√©quence")
    print(f"   3Ô∏è‚É£ Je d√©cide: {action_names[action_int]}")

    # Execute
    obs, reward, done, info = vec_env.step(action)

    # After
    pos_after = raw_env.position_side
    balance_after = raw_env.balance

    print(f"   4Ô∏è‚É£ L'environnement ex√©cute mon action")
    print(f"   5Ô∏è‚É£ Je re√ßois reward: {reward[0]:+.4f}")
    print(f"")
    print(f"   üìä R√©sultat:")
    print(f"   ‚îú‚îÄ‚îÄ Position: {pos_before} ‚Üí {pos_after}")
    print(f"   ‚îú‚îÄ‚îÄ Balance: ${balance_before:,.2f} ‚Üí ${balance_after:,.2f}")
    print(f"   ‚îî‚îÄ‚îÄ Changement: ${balance_after - balance_before:+.2f}")

    if pos_before == 0 and pos_after != 0:
        print(f"\n   üéØ J'ai OUVERT une position!")
    elif pos_before != 0 and pos_after == 0:
        print(f"\n   üèÅ J'ai FERM√â ma position!")
    elif action_int == 1:
        print(f"\n   ‚è∏Ô∏è J'ai choisi de ne rien faire (HOLD)")

def ask_blockers():
    """Demande: Qu'est-ce qui t'emp√™che de trader?"""
    global raw_env

    print("\n" + "-"*50)
    print("‚ùì QUESTION: Qu'est-ce qui t'emp√™che de trader?")
    print("-"*50)

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   V√©rification des blocages...")
    print(f"")

    blockers = []

    # Daily loss
    if hasattr(raw_env, 'daily_loss_limit_reached') and raw_env.daily_loss_limit_reached:
        blockers.append("üö´ Daily Loss Limit atteint!")
    else:
        print(f"   ‚úÖ Daily Loss Limit: OK")

    # Max drawdown
    if hasattr(raw_env, 'max_drawdown'):
        dd = raw_env.max_drawdown
        if dd >= 0.10:
            blockers.append(f"üö´ Max Drawdown >= 10% ({dd*100:.1f}%)")
        elif dd >= 0.07:
            print(f"   ‚ö†Ô∏è Drawdown √©lev√©: {dd*100:.1f}% (attention!)")
        else:
            print(f"   ‚úÖ Drawdown: {dd*100:.1f}% (OK)")

    # Tail risk
    if hasattr(raw_env, 'tail_risk_detected') and raw_env.tail_risk_detected:
        blockers.append("üö´ Tail Risk d√©tect√©!")
    else:
        print(f"   ‚úÖ Tail Risk: Non d√©tect√©")

    # Confidence threshold
    if hasattr(raw_env, '_calculate_min_confidence_threshold'):
        threshold = raw_env._calculate_min_confidence_threshold()
        print(f"   ‚ÑπÔ∏è Seuil de confiance: {threshold*100:.0f}%")
        print(f"      (Pour discrete, size=100% donc pas de blocage)")

    if blockers:
        print(f"\n   üö® BLOCAGES ACTIFS:")
        for b in blockers:
            print(f"   {b}")
    else:
        print(f"\n   ‚úÖ Aucun blocage technique!")
        print(f"   Si je ne trade pas, c'est par CHOIX")

def ask_improve():
    """Demande: Comment puis-je t'aider √† trader plus?"""
    print("\n" + "-"*50)
    print("‚ùì QUESTION: Comment puis-je t'aider √† trader plus?")
    print("-"*50)

    print(f"\nü§ñ R√âPONSE DE L'AGENT:")
    print(f"")
    print(f"   üí° Pour me faire trader plus, il faudrait:")
    print(f"")
    print(f"   1. üé≤ PLUS D'EXPLORATION")
    print(f"      Augmenter mon entropy coefficient")
    print(f"      Actuellement: 0.25 (d√©j√† augment√©!)")
    print(f"")
    print(f"   2. üéÅ R√âCOMPENSER MES TRADES")
    print(f"      Ajouter un bonus quand je BUY ou SELL")
    print(f"      Actuellement: +0.03 par action (d√©j√† ajout√©!)")
    print(f"")
    print(f"   3. üò§ P√âNALISER MON INACTION")
    print(f"      P√©naliser si je fais trop de HOLD")
    print(f"      Actuellement: -0.02 si >30 HOLDs (d√©j√† ajout√©!)")
    print(f"")
    print(f"   4. üß† ME DONNER DE L'EXP√âRIENCE")
    print(f"      Pr√©-remplir ma m√©moire avec des trades")
    print(f"      Pour √©viter le cold start")
    print(f"")
    print(f"   üìã FIXES D√âJ√Ä IMPL√âMENT√âS:")
    print(f"   ‚úÖ Entropy √©tendue: 0-50% √† 0.25")
    print(f"   ‚úÖ Entropy minimum: 0.12 (au lieu de 0.05)")
    print(f"   ‚úÖ Exploration bonus: +0.03")
    print(f"   ‚úÖ HOLD penalty: -0.02")

# ============================================================================
# MAIN MENU
# ============================================================================

def main_menu():
    print_header()
    print(f"\n   Mod√®le: {model_name}")
    print(f"   Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"\n" + "="*70)
    print("\n   QUESTIONS DISPONIBLES:")
    print(f"")
    print(f"   [1] üéØ Quelle action vas-tu prendre?")
    print(f"   [2] ü§î Pourquoi cette d√©cision?")
    print(f"   [3] üò∞ As-tu peur de perdre?")
    print(f"   [4] üìä Quelle est ta position actuelle?")
    print(f"   [5] üß† Que retiens-tu de tes trades?")
    print(f"   [6] ‚ñ∂Ô∏è  Montre-moi un step complet")
    print(f"   [7] üö´ Qu'est-ce qui t'emp√™che de trader?")
    print(f"   [8] üí° Comment t'aider √† trader plus?")
    print(f"")
    print(f"   [A] üîÑ Toutes les questions (automatique)")
    print(f"   [R] üîÅ Reset l'environnement")
    print(f"   [Q] üö™ Quitter")
    print(f"\n" + "="*70)

def run_all_questions():
    """Ex√©cute toutes les questions automatiquement"""
    print("\n" + "="*70)
    print("   INTERVIEW COMPL√àTE - TOUTES LES QUESTIONS")
    print("="*70)

    ask_action()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_why()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_fear()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_position()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_memory()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_execute_step()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_blockers()
    input("\n   [Appuyez sur Entr√©e pour continuer...]")

    ask_improve()

# ============================================================================
# MAIN LOOP
# ============================================================================

if __name__ == "__main__":
    while True:
        main_menu()

        choice = input("\n   Votre choix: ").strip().upper()

        if choice == '1':
            ask_action()
        elif choice == '2':
            ask_why()
        elif choice == '3':
            ask_fear()
        elif choice == '4':
            ask_position()
        elif choice == '5':
            ask_memory()
        elif choice == '6':
            ask_execute_step()
        elif choice == '7':
            ask_blockers()
        elif choice == '8':
            ask_improve()
        elif choice == 'A':
            run_all_questions()
        elif choice == 'R':
            obs = vec_env.reset()
            lstm_states = None
            print("\n   ‚úÖ Environnement r√©initialis√©!")
        elif choice == 'Q':
            print("\n   üëã Au revoir!")
            break
        else:
            print("\n   ‚ùå Choix invalide, r√©essayez.")

        input("\n   [Appuyez sur Entr√©e pour continuer...]")

# Cleanup
vec_env.close()
